@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@article{DBLP:journals/corr/GehringAGYD17,
  author    = {Jonas Gehring and
               Michael Auli and
               David Grangier and
               Denis Yarats and
               Yann N. Dauphin},
  title     = {Convolutional Sequence to Sequence Learning},
  journal   = {CoRR},
  volume    = {abs/1705.03122},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.03122},
  archivePrefix = {arXiv},
  eprint    = {1705.03122},
  timestamp = {Mon, 13 Aug 2018 16:48:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GehringAGYD17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/KalchbrennerESO16,
  author    = {Nal Kalchbrenner and
               Lasse Espeholt and
               Karen Simonyan and
               A{\"{a}}ron van den Oord and
               Alex Graves and
               Koray Kavukcuoglu},
  title     = {Neural Machine Translation in Linear Time},
  journal   = {CoRR},
  volume    = {abs/1610.10099},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.10099},
  archivePrefix = {arXiv},
  eprint    = {1610.10099},
  timestamp = {Mon, 13 Aug 2018 16:46:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KalchbrennerESO16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{NIPS2016_6295,
title = {Can Active Memory Replace Attention?},
author = {Kaiser, \L ukasz and Bengio, Samy},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {3781--3789},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6295-can-active-memory-replace-attention.pdf}
}

@misc{ba2016layer,
    title={Layer Normalization},
    author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year={2016},
    eprint={1607.06450},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}


@article{DBLP:journals/corr/HeZRS15,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{bahdanau2014neural,
    title={Neural Machine Translation by Jointly Learning to Align and Translate},
    author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
    year={2014},
    eprint={1409.0473},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{DBLP:journals/corr/WuSCLNMKCGMKSJL16,
  author    = {Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  title     = {Google's Neural Machine Translation System: Bridging the Gap between
               Human and Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1609.08144},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.08144},
  archivePrefix = {arXiv},
  eprint    = {1609.08144},
  timestamp = {Thu, 14 Mar 2019 09:34:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/WuSCLNMKCGMKSJL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{vaswani2017attention,
    title={Attention Is All You Need},
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2017},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{zhu2015aligning,
    title={Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},
    author={Yukun Zhu and Ryan Kiros and Richard Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},
    year={2015},
    eprint={1506.06724},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}


@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}


@inproceedings{lee-etal-2017-interactive,
    title = "Interactive Visualization and Manipulation of Attention-based Neural Machine Translation",
    author = "Lee, Jaesong  and
      Shin, Joong-Hwi  and
      Kim, Jun-Seok",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-2021",
    doi = "10.18653/v1/D17-2021",
    pages = "121--126",
    abstract = "While neural machine translation (NMT) provides high-quality translation, it is still hard to interpret and analyze its behavior. We present an interactive interface for visualizing and intervening behavior of NMT, specifically concentrating on the behavior of beam search mechanism and attention component. The tool (1) visualizes search tree and attention and (2) provides interface to adjust search tree and attention weight (manually or automatically) at real-time. We show the tool gives various methods to understand NMT.",
}


@inproceedings{yang-etal-2017-satirical,
    title = "Satirical News Detection and Analysis using Attention Mechanism and Linguistic Features",
    author = "Yang, Fan  and
      Mukherjee, Arjun  and
      Dragut, Eduard",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1211",
    doi = "10.18653/v1/D17-1211",
    pages = "1979--1989",
    abstract = "Satirical news is considered to be entertainment, but it is potentially deceptive and harmful. Despite the embedded genre in the article, not everyone can recognize the satirical cues and therefore believe the news as true news. We observe that satirical cues are often reflected in certain paragraphs rather than the whole document. Existing works only consider document-level features to detect the satire, which could be limited. We consider paragraph-level linguistic features to unveil the satire by incorporating neural network and attention mechanism. We investigate the difference between paragraph-level features and document-level features, and analyze them on a large satirical news dataset. The evaluation shows that the proposed model detects satirical news effectively and reveals what features are important at which level.",
}


@misc{serrano2019attention,
    title={Is Attention Interpretable?},
    author={Sofia Serrano and Noah A. Smith},
    year={2019},
    eprint={1906.03731},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}



@misc{jain2019attention,
    title={Attention is not Explanation},
    author={Sarthak Jain and Byron C. Wallace},
    year={2019},
    eprint={1902.10186},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{wiegreffe2019attention,
    title={Attention is not not Explanation},
    author={Sarah Wiegreffe and Yuval Pinter},
    year={2019},
    eprint={1908.04626},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{mller2020covidtwitterbert,
    title={COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter},
    author={Martin Müller and Marcel Salathé and Per E Kummervold},
    year={2020},
    eprint={2005.07503},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{liu2020crisisbert,
    title={CrisisBERT: a Robust Transformer for Crisis Classification and Contextual Crisis Embedding},
    author={Junhua Liu and Trisha Singhal and Lucienne T. M. Blessing and Kristin L. Wood and Kwan Hui Lim},
    year={2020},
    eprint={2005.06627},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{DBLP:journals/corr/KuchaievG17,
  author    = {Oleksii Kuchaiev and
               Boris Ginsburg},
  title     = {Factorization tricks for {LSTM} networks},
  journal   = {CoRR},
  volume    = {abs/1703.10722},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.10722},
  archivePrefix = {arXiv},
  eprint    = {1703.10722},
  timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KuchaievG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@article{radford2018improving,
  title={Improving language understanding with unsupervised learning},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={Technical report, OpenAI},
  year={2018}
}

@article{DBLP:journals/corr/abs-1906-02715,
  author    = {Andy Coenen and
               Emily Reif and
               Ann Yuan and
               Been Kim and
               Adam Pearce and
               Fernanda B. Vi{\'{e}}gas and
               Martin Wattenberg},
  title     = {Visualizing and Measuring the Geometry of {BERT}},
  journal   = {CoRR},
  volume    = {abs/1906.02715},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.02715},
  archivePrefix = {arXiv},
  eprint    = {1906.02715},
  timestamp = {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-02715.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}